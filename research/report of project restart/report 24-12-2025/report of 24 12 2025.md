# Report of 24 12 2025 (Consolidated Report Pack)

**Project:** Real-time Facial Expression Recognition System (FER) – Restart/Reconstruction Phase  
**Date:** 2025-12-24  
**Scope:** Consolidation + comparison of the 10 mini-reports under `research/report 24-12-2025/report md file/`.

## Abstract

This report documents the December 2025 checkpoint of a reproducible teacher→ensemble→softlabels→student pipeline for real-time facial expression recognition (FER). Strong offline teachers (RN18, B3, CNXT) are trained and compared, then combined via a weighted logit-space ensemble chosen on a mixed-source robustness benchmark. Ensemble outputs are exported as softlabels so student training does not require running teachers. A deployable student (MobileNetV3-Large) is trained in staged CE → KD → DKD modes while tracking both classification (accuracy, macro-F1, per-class F1) and reliability (NLL/ECE, raw and temperature-scaled) metrics.

---

## Table of Contents

1. Executive Summary
2. Introduction and Background
3. Problem Context
4. Problem Definition
5. Canonical Baselines (this report’s scope)
6. Objectives (Dec 2025 checkpoint)
7. What This Report Combines
8. Data Cleaning and Manifests (04 + 05)
9. Evaluation Protocol (08)
10. Teacher Training (01)
11. Ensemble / Softlabels (02)
12. Student Training: CE → KD → DKD (03)
13. Cross-Component Comparison (teachers vs ensemble vs student)
14. Reproducibility and Alignment (07)
15. Real-time Demo and Pipeline (06 + 09)
16. NL / NegL Status (10)
17. Conclusions and Next Steps
18. Artifact Map (paths)
19. Appendix: Mathematical Formulations

---

## 1) Executive Summary

This report consolidates the **Dec-24-2025** report pack and compares the current pipeline stages:

- **Dataset integrity is strong:** `Training_data_cleaned/classification_manifest.csv` validates to **466,284** rows with **0 missing paths** and **0 bad labels** (see `outputs/manifest_validation_all_with_expw.json`).
- **Stage-A teachers are strong on their recorded validation split** (val=18,165 after filtering) with **macro-F1 ~0.781–0.791**.
- **Best mixed-source ensemble choice** (selected by `test_all_sources.csv`, n=48,928) is **RN18/B3/CNXT = 0.4/0.4/0.2** with **macro-F1 0.659608**.
- **Student results (HQ-train test split, test=27,840):**
  - CE: **macro-F1 0.741952** (best macro-F1 among CE/KD/DKD in this run)
  - KD: **macro-F1 0.733351**
  - DKD: **macro-F1 0.737511**
  - KD/DKD significantly improve **temperature-scaled calibration** (ECE ≈ 0.027 vs CE ≈ 0.050).
- **Known incident resolved:** DKD previously produced an “empty output” due to resume epoch logic; PowerShell runner was patched so DKD trains additional epochs after resuming.
- **Real-time pipeline exists and logs CSV**, but **timed KPIs (FPS/latency/flip-rate) are not claimed** until a timed demo run is captured.

Important comparison caveat:
- Teachers, ensemble selection, and student runs are evaluated on **different manifests** in this report pack. Comparisons are reported with their evaluation source explicitly stated.

---

## 2) Introduction and Background

Real-time FER aims to classify a person’s facial expression from video frames under strict latency and compute constraints. The key tension is that high-capacity models (and especially ensembles) can achieve better accuracy and robustness but are often too heavy for real-time deployment.

This motivates the teacher→student approach used in this repo:

- Train strong teachers offline.
- Select a weighted ensemble on a harder mixed-source benchmark.
- Export ensemble softlabels (logits) for distillation.
- Train a compact student for deployment.
- Close the offline–online gap with consistent preprocessing and temporal stabilization.

## 3) Problem Context

Key challenges for deployment-oriented FER in this project:

- **Imbalance and hard classes:** minority/subtle classes (e.g., Fear, Disgust) are frequently confused.
- **Calibration:** overconfident errors reduce trust and complicate downstream decision making.
- **Domain shift:** offline static-image performance may not match webcam performance due to lighting, camera noise, and temporal dynamics.
- **Reproducibility:** multi-stage pipelines (cleaning → training → softlabel export → distillation → demo) require explicit evidence to avoid silent drift.

## 4) Problem Definition

This project targets a practical deployment question:

- **Input:** a live video stream (webcam/video) containing a detectable face.
- **Output:** one of 7 canonical expressions (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral), updated in real time.
- **Constraints:** low latency, stable (non-flickering) predictions, and reliable confidence scores (calibration).

## 5) Canonical Baselines (this report’s scope)

Important note: teachers, ensembles, and student stages are evaluated on different manifests depending on the component. Every table below explicitly states the evaluation source.

Baselines used in this report pack:

- Teacher (single-model) summary: Stage A validation split from `Training_data_cleaned/classification_manifest.csv` after filtering; val=18,165.
- Ensemble selection summary: mixed-source benchmark `Training_data_cleaned/test_all_sources.csv`, n=48,928.
- Student summary: HQ-train evaluation from `Training_data_cleaned/classification_manifest_hq_train.csv` test split; test=27,840.

## 6) Objectives (Dec 2025 checkpoint)

- Keep all experiments reproducible (saved manifests, run folders, checkpoints, metrics JSONs).
- Use a single canonical 7-class mapping across all datasets.
- Select teacher ensemble weights based on a mixed-source benchmark rather than a single dataset split.
- Produce a student that is deployable and well-calibrated (report raw + temperature-scaled NLL/ECE).

## 2) What This Report Combines

This document combines and compares the following source reports:

- Index: `research/report 24-12-2025/report md file/00_index.md`
- Teacher training: `.../01_teacher_model_training_report.md`
- Ensemble/softlabels: `.../02_teacher_model_ensemble_report.md`
- Student CE/KD/DKD: `.../03_student_model_training_report.md`
- Data cleaning: `.../04_dataset_cleaning_report.md`
- Data usage: `.../05_dataset_usage_report.md`
- Basic demo: `.../06_basic_demo_report.md`
- Alignment/repro: `.../07_alignment_reproducibility_report.md`
- Evaluation protocol: `.../08_evaluation_protocol_report.md`
- Real-time pipeline: `.../09_realtime_pipeline_report.md`
- NL/NegL note: `.../10_nl_negl_research_note.md`

---

## 3) Data Cleaning and Manifests (04 + 05)

### 3.1 Canonical label space

All datasets are normalized into a single 7-class label space:

- `Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral`

### 3.2 Key outputs

- Cleaning provenance: `Training_data_cleaned/clean_report.json`
- Main unified manifest: `Training_data_cleaned/classification_manifest.csv`
- HQ training manifest: `Training_data_cleaned/classification_manifest_hq_train.csv`
- ExpW manifests: `Training_data_cleaned/expw_full_manifest.csv`, `Training_data_cleaned/expw_hq_manifest.csv`
- Validation reports:
  - `outputs/manifest_validation.json`
  - `outputs/manifest_validation_all_with_expw.json`

### 3.3 Validation results (hard gate)

From `outputs/manifest_validation_all_with_expw.json`:

- Total rows: **466,284**
- Valid rows: **466,284**
- Missing paths: **0**
- Bad labels: **0**
- Decode sample: attempted **300**, ok **300**, fail **0**

### 3.4 Source composition (rows)

From `outputs/manifest_validation_all_with_expw.json` `counts.by_source`:

- `fer2013_uniform_7`: **140,000**
- `ferplus`: **138,526**
- `expw_full`: **91,793**
- `affectnet_full_balanced`: **71,764**
- `rafdb_basic`: **15,339**
- `rafml_argmax`: **4,908**
- `rafdb_compound_mapped`: **3,954**

### 3.5 Manifest usage (recommended)

From the report pack:

- Teacher / student training (clean supervision): `classification_manifest_hq_train.csv`
- Mixed-source robustness benchmark (ensemble selection): `test_all_sources.csv`
- Broader experiments / maximum coverage: `classification_manifest.csv`

---

## 4) Evaluation Protocol (08)

### 4.1 Evaluation sources used in this report pack

- HQ-train evaluation: `Training_data_cleaned/classification_manifest_hq_train.csv`
- Mixed-source test: `Training_data_cleaned/test_all_sources.csv`
- RAF-only: `Training_data_cleaned/rafdb_basic_only.csv` and other RAF CSVs

### 4.2 Metrics (per run)

The pack standardizes storing metrics under each run folder as JSON:

- Accuracy
- Macro-F1
- Per-class F1
- Calibration: NLL, ECE, Brier (where emitted)
- Temperature scaling: global temperature + post-calibration NLL/ECE

Helpers referenced in the pack:

- `scripts/compute_reliability.py`
- `scripts/score_live_results.py` (for live/demo logs)

---

## 5) Teacher Training (01)

### 5.1 Objective

Train strong “teacher” models (Stage A) and export full run provenance for later ensemble + student distillation.

### 5.2 Architectures (Stage A, img224)

- RN18: `resnet18`
- B3: `tf_efficientnet_b3`
- CNXT: `convnext_tiny`

### 5.3 Training configuration (from checkpoint args)

Common Stage-A settings (from the report pack):

- Image size: **224**
- LR: **3e-4**, Min LR: **1e-5**, Weight decay: **0.05**
- Epochs: **60**
- Batch size: **64**
- CLAHE: enabled
- ArcFace: `m=0.35`, `s=30.0` (plain logits warmup and margin ramp)

### 5.4 Teacher evaluation dataset (IMPORTANT)

Teachers are evaluated on the validation split recorded by each run’s `alignmentreport.json`:

- Based on `Training_data_cleaned/classification_manifest.csv`
- After source filtering:
  - Total after filter: **225,629**
  - Train: **182,960**
  - Val: **18,165** (metrics below are on this val split)

### 5.5 Final teacher metrics (val=18,165)

From each teacher run’s `reliabilitymetrics.json`:

| Teacher | Accuracy | Macro-F1 | Raw NLL | Temp NLL | Raw ECE | Temp ECE |
| --- | ---: | ---: | ---: | ---: | ---: | ---: |
| RN18 | 0.7862 | 0.7808 | 4.0259 | 0.8803 | 0.2053 | 0.1489 |
| B3 | 0.7961 | 0.7910 | 3.2219 | 0.7871 | 0.1988 | 0.0839 |
| CNXT | 0.7941 | 0.7890 | 3.1014 | 0.7700 | 0.2009 | 0.0817 |

Run folders:

- `outputs/teachers/RN18_resnet18_seed1337_stageA_img224/`
- `outputs/teachers/B3_tf_efficientnet_b3_seed1337_pretrained_true_v1_stageA_img224/`
- `outputs/teachers/CNXT_convnext_tiny_seed1337_stageA_img224/`

---

## 6) Ensemble / Softlabels (02)

### 6.1 Method

- Fusion: weighted **logit fusion** (sum logits → softmax)
- Preprocessing: **CLAHE enabled**
- Weights selected by benchmark performance

### 6.2 Key benchmark for selection

Primary robustness benchmark:

- `Training_data_cleaned/test_all_sources.csv` (n=48,928)

Best 3-teacher ensemble (selected):

- **RN18/B3/CNXT = 0.4/0.4/0.2**
- acc=**0.687255** | macro-F1=**0.659608**
- NLL=**4.077156** | ECE=**0.287694** | Brier=**0.590869**

Per-class F1 (test_all_sources):

- Angry 0.6304, Disgust 0.6034, Fear 0.5350, Happy 0.8389, Sad 0.5924, Surprise 0.7205, Neutral 0.6967

Note: this benchmark is harder than the teacher’s internal val split and is intended to reflect mixed-domain deployment realism.

### 6.3 Selected softlabels export used for student KD/DKD

- `outputs/softlabels/_ens_hq_train_rn18_0p4_b3_0p4_cnxt_0p2_logit_clahe_20251223_152856/`

Expected files:

- `softlabels.npz`
- `softlabels_index.jsonl`
- `hash_manifest.json`
- `classorder.json`
- `alignmentreport.json`
- `ensemble_metrics.json`

### 6.4 Archived benchmark artifact (for the selection number)

The specific benchmark metrics for test_all_sources 0.4/0.4/0.2 were recorded under:

- `outputs/softlabels/_archive/bad_list_20251223_121501/_ens_test_all_sources_rn18_0p4_b3_0p4_cnxt_0p2_logit_clahe_20251223_111523/ensemble_metrics.json`

---

## 7) Student Training: CE → KD → DKD (03)

### 7.1 Student architecture and run folders

- Student: MobileNetV3-Large (`mobilenetv3_large_100` via timm)
- Input: img224
- CLAHE enabled; AMP enabled (as per the pack’s student run)

Run folders:

- CE: `outputs/students/mobilenetv3_large_100_img224_seed1337_CE_20251223_225031/`
- KD: `outputs/students/mobilenetv3_large_100_img224_seed1337_KD_20251223_225031/`
- DKD: `outputs/students/mobilenetv3_large_100_img224_seed1337_DKD_20251223_225031/`
- Logs: `outputs/students/_logs_20251223_225031/`

### 7.2 Supervision source for KD/DKD

KD/DKD consume exported logits (softlabels) rather than executing teachers at student training time:

- `outputs/softlabels/_ens_hq_train_rn18_0p4_b3_0p4_cnxt_0p2_logit_clahe_20251223_152856/`
  - `softlabels.npz`
  - `softlabels_index.jsonl`

### 7.3 Training settings

- KD: T=2, α=0.5, 20 epochs
- DKD: T=2, α=0.5, β=4, resume from KD best, then train 10 additional epochs (after runner fix)

### 7.4 Student evaluation dataset (IMPORTANT)

Students are evaluated on the HQ-train manifest splits:

From `Training_data_cleaned/classification_manifest_hq_train.csv` (as recorded in the report pack):

- Train: **213,144**
- Val: **18,020**
- Test: **27,840**
- Total: **259,004**

### 7.5 Final student metrics (HQ-train test=27,840)

From each student run’s `reliabilitymetrics.json`:

| Student stage | Accuracy | Macro-F1 | Raw NLL | Temp NLL | Raw ECE | Temp ECE | Global T |
| --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| CE | 0.750174 | 0.741952 | 1.315335 | 0.777757 | 0.131019 | 0.049897 | 3.228 |
| KD | 0.734688 | 0.733351 | 2.093148 | 0.768196 | 0.215289 | 0.027764 | 5.000 |
| DKD | 0.737432 | 0.737511 | 1.511788 | 0.765203 | 0.209450 | 0.026605 | 3.348 |

Interpretation (from the report pack):

- CE currently has the best macro-F1 on HQ-train.
- KD/DKD improve post-hoc calibration (temperature-scaled ECE/NLL), but raw calibration is worse than CE.

---

## 8) Cross-Component Comparison (teachers vs ensemble vs student)

### 8.1 Why a direct “who is best” comparison is not valid (yet)

These results are measured on different evaluation sets:

- Teacher metrics are on the teacher run’s internal validation split (val=18,165 after filtering).
- Ensemble selection is based on `test_all_sources.csv` (n=48,928).
- Student metrics are on HQ-train test split (test=27,840).

Therefore, this report compares them **within their evaluation context** and calls out what should be compared next.

### 8.2 Summary table (with evaluation source)

| Component | Model | Accuracy | Macro-F1 | Evaluation source |
| --- | --- | ---: | ---: | --- |
| Teacher | RN18 | 0.7862 | 0.7808 | teacher val split (18,165) |
| Teacher | B3 | 0.7961 | 0.7910 | teacher val split (18,165) |
| Teacher | CNXT | 0.7941 | 0.7890 | teacher val split (18,165) |
| Ensemble | RN18/B3/CNXT 0.4/0.4/0.2 | 0.6873 | 0.6596 | `test_all_sources.csv` (48,928) |
| Student | CE | 0.7502 | 0.7420 | HQ-train test (27,840) |
| Student | KD | 0.7347 | 0.7334 | HQ-train test (27,840) |
| Student | DKD | 0.7374 | 0.7375 | HQ-train test (27,840) |

### 8.3 Main comparison findings

- **Teachers:** B3 slightly leads in macro-F1 on the teacher val split.
- **Ensemble:** chosen to optimize mixed-source robustness; its absolute macro-F1 is lower because the benchmark is harder/more diverse.
- **Student:** CE currently leads macro-F1 in this run; KD/DKD help calibration post-hoc.

### 8.4 Required next comparison (actionable)

To make an apples-to-apples comparison:

- Evaluate **student CE/KD/DKD** checkpoints on `test_all_sources.csv` (same benchmark used to pick ensemble weights).
- Optionally evaluate teachers on HQ-train test if that is the intended deployment distribution.

---

## 9) Reproducibility and Alignment (07)

### 9.1 Provenance artifacts

- Manifest validation:
  - `outputs/manifest_validation_all_with_expw.json`
  - `outputs/manifest_validation.json`
- Cleaning provenance: `Training_data_cleaned/clean_report.json`
- Softlabels have explicit indexing:
  - `softlabels.npz` + `softlabels_index.jsonl`
- Checkpoint provenance tools:
  - `scripts/inspect_checkpoint_provenance.py`
  - `_ckpt_provenance_*.txt`

### 9.2 DKD resume incident (resolved)

- Root cause: DKD `start_epoch` > configured total epochs ⇒ training loop did nothing.
- Fix: runner computes DKD total epochs as `(resume_epoch + 1 + extra_dkd_epochs)`.
- Impact: DKD outputs are now produced reliably.

---

## 10) Real-time Demo and Pipeline (06 + 09)

### 10.1 Demo entrypoints

- Demo: `demo/realtime_demo.py`
- Related inference helper: `scripts/realtime_infer_arcface.py`

### 10.2 Pipeline stages

1. Frame capture
2. Face detection
3. Crop + resize to img224
4. Preprocessing (CLAHE when enabled)
5. FER inference
6. Temporal smoothing (EMA/hysteresis/voting)
7. Visualization + CSV logging

Outputs:

- CSV logs in `demo/outputs/`

### 10.3 KPI status

No timed demo CSV is attached in this report pack, therefore **FPS / latency / flip-rate are not claimed yet**.

Recommended measurement step (from the pack):

- Run demo 2–3 minutes on target machine.
- Compute FPS, latency distribution, and label flip-rate from `demo/outputs/*.csv`.

---

## 11) NL / NegL Status (10)

- NL/NegL is currently documented as research notes under `research/nl_negl/`.
- No integrated NL/NegL training result is claimed in this pack.

Proposed future success criteria (from the pack):

- Macro-F1 improvement vs CE and/or KD/DKD on chosen benchmark(s)
- Calibration stable or improved (post-temperature scaling)
- Reduced confusion on hard pairs (Fear↔Surprise, Sad↔Neutral)
- Full reproducibility maintained

---

## 12) Conclusions and Next Steps

### 12.1 What is complete (Dec 24, 2025)

- Cleaned, validated manifests across multiple datasets and unified 7-class mapping.
- Strong Stage-A teachers with full provenance.
- Evidence-based ensemble selection on mixed-source benchmark.
- Student CE/KD/DKD pipeline runs end-to-end with consistent outputs.
- DKD resume bug fixed and validated.

### 12.2 What must be done next (highest value)

1. Evaluate CE/KD/DKD students on `test_all_sources.csv` to measure mixed-source generalization.
2. Run timed demo and produce KPI summary (FPS/latency/flip-rate) from CSV logs.
3. Tune KD/DKD hyperparameters and/or training duration to seek macro-F1 gains without losing calibration.
4. Define a minimal NL/NegL experiment matrix and run first measurable baselines.

---

## 13) Artifact Map (paths)

### 13.1 Reports

- Report pack index: `research/report 24-12-2025/report md file/00_index.md`
- Mini-reports: `research/report 24-12-2025/report md file/01_...` through `10_...`

### 13.2 Data + validation

- `Training_data_cleaned/classification_manifest.csv`
- `Training_data_cleaned/classification_manifest_hq_train.csv`
- `Training_data_cleaned/test_all_sources.csv`
- `Training_data_cleaned/clean_report.json`
- `outputs/manifest_validation_all_with_expw.json`

### 13.3 Teachers

- `outputs/teachers/RN18_resnet18_seed1337_stageA_img224/`
- `outputs/teachers/B3_tf_efficientnet_b3_seed1337_pretrained_true_v1_stageA_img224/`
- `outputs/teachers/CNXT_convnext_tiny_seed1337_stageA_img224/`

### 13.4 Ensembles / softlabels

- Selected softlabels: `outputs/softlabels/_ens_hq_train_rn18_0p4_b3_0p4_cnxt_0p2_logit_clahe_20251223_152856/`
- Selection benchmark artifact (archived): `outputs/softlabels/_archive/bad_list_20251223_121501/_ens_test_all_sources_rn18_0p4_b3_0p4_cnxt_0p2_logit_clahe_20251223_111523/ensemble_metrics.json`

### 13.5 Students

- `outputs/students/mobilenetv3_large_100_img224_seed1337_CE_20251223_225031/`
- `outputs/students/mobilenetv3_large_100_img224_seed1337_KD_20251223_225031/`
- `outputs/students/mobilenetv3_large_100_img224_seed1337_DKD_20251223_225031/`

### 13.6 Demo

- `demo/realtime_demo.py`
- `demo/outputs/`

---

## 19) Appendix: Mathematical Formulations

### Knowledge Distillation (KD)

The classical KD objective used in the report write-up:

$$
\mathcal{L}_{KD} = (1-\alpha)\,\mathcal{L}_{CE}(y, \sigma(z_s)) + \alpha T^2 \, KL\big(\sigma(z_t/T)\,\|\,\sigma(z_s/T)\big)
$$

where $z_t$ are teacher logits, $z_s$ are student logits, $T$ is temperature, and $\sigma$ is softmax.

### Decoupled Knowledge Distillation (DKD)

The DKD objective (target-class knowledge + non-target-class knowledge):

$$
\mathcal{L}_{DKD} = (1-\alpha)\,\mathcal{L}_{CE} + \alpha T^2\, \mathcal{L}_{TCKD} + \beta T^2\, \mathcal{L}_{NCKD}
$$

### ArcFace loss (teacher training protocol)

The ArcFace-style objective referenced in the report (project configuration: $m=0.35$, $s=30.0$):

$$
\mathcal{L}_{ArcFace} = -\frac{1}{n}\sum_{i=1}^n \log
\frac{e^{s\cos(\theta_{y_i}+m)}}{e^{s\cos(\theta_{y_i}+m)} + \sum_{j \ne y_i} e^{s\cos(\theta_j)}}
$$
